{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 4)\n",
      "(54, 4)\n",
      "(44, 4)\n",
      "(24, 4)\n"
     ]
    }
   ],
   "source": [
    "filenames=[\"6_3\",\"06_9\",\"06_16 \",\"06_16\"]\n",
    "raw_df=pd.DataFrame()\n",
    "for file in filenames:\n",
    "    a=pd.read_json(\"datasets/{}.json\".format(file),lines=True)\n",
    "    print(a.shape)\n",
    "    raw_df = pd.concat([raw_df,pd.read_json(\"datasets/{}.json\".format(file),lines=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.reset_index(inplace=True)\n",
    "raw_df.drop(columns=[\"index\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that the username in each case is always \"admin\" for all the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1835</td>\n",
       "      <td>1 The applicants Sharman Networks Ltd ('Sharma...</td>\n",
       "      <td>[[17, 37, Organization], [38, 58, Organization...</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1836</td>\n",
       "      <td>When referring to the applicants generally, I ...</td>\n",
       "      <td>[[60, 84, person]]</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1837</td>\n",
       "      <td>Each of the Sharman applicants was one of ten ...</td>\n",
       "      <td>[[12, 30, person], [134, 157, Organization], [...</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1838</td>\n",
       "      <td>Wilcox J made orders ancillary to the Mareva o...</td>\n",
       "      <td>[[55, 68, Date], [91, 109, person], [0, 8, Jud...</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1839</td>\n",
       "      <td>2 Wilcox J delivered judgment on the complex i...</td>\n",
       "      <td>[[103, 119, Date], [122, 186, Versus], [2, 10,...</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>3160</td>\n",
       "      <td>I think that is sensible given that it is self...</td>\n",
       "      <td>[[77, 86, Location], [123, 137, person]]</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>3161</td>\n",
       "      <td>Moreover, once removed from Australia, such en...</td>\n",
       "      <td>[[28, 38, Location], [83, 93, Location]]</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>3162</td>\n",
       "      <td>12 I therefore propose to make an order restra...</td>\n",
       "      <td>[[107, 116, Location]]</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>3165</td>\n",
       "      <td>If he is to be removed from Australia notwiths...</td>\n",
       "      <td>[[28, 37, Location]]</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>3170</td>\n",
       "      <td>I certify that the preceding thirteen (13) num...</td>\n",
       "      <td>[[121, 150, person]]</td>\n",
       "      <td>admin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                                               text  \\\n",
       "0      1835  1 The applicants Sharman Networks Ltd ('Sharma...   \n",
       "1      1836  When referring to the applicants generally, I ...   \n",
       "2      1837  Each of the Sharman applicants was one of ten ...   \n",
       "3      1838  Wilcox J made orders ancillary to the Mareva o...   \n",
       "4      1839  2 Wilcox J delivered judgment on the complex i...   \n",
       "..      ...                                                ...   \n",
       "281    3160  I think that is sensible given that it is self...   \n",
       "282    3161  Moreover, once removed from Australia, such en...   \n",
       "283    3162  12 I therefore propose to make an order restra...   \n",
       "284    3165  If he is to be removed from Australia notwiths...   \n",
       "285    3170  I certify that the preceding thirteen (13) num...   \n",
       "\n",
       "                                              entities username  \n",
       "0    [[17, 37, Organization], [38, 58, Organization...    admin  \n",
       "1                                   [[60, 84, person]]    admin  \n",
       "2    [[12, 30, person], [134, 157, Organization], [...    admin  \n",
       "3    [[55, 68, Date], [91, 109, person], [0, 8, Jud...    admin  \n",
       "4    [[103, 119, Date], [122, 186, Versus], [2, 10,...    admin  \n",
       "..                                                 ...      ...  \n",
       "281           [[77, 86, Location], [123, 137, person]]    admin  \n",
       "282           [[28, 38, Location], [83, 93, Location]]    admin  \n",
       "283                             [[107, 116, Location]]    admin  \n",
       "284                               [[28, 37, Location]]    admin  \n",
       "285                               [[121, 150, person]]    admin  \n",
       "\n",
       "[286 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['admin'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[\"username\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.drop(columns=[\"username\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[\"doc_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3136    2\n",
       "3165    2\n",
       "3161    2\n",
       "3155    2\n",
       "3153    2\n",
       "       ..\n",
       "2000    1\n",
       "2001    1\n",
       "2002    1\n",
       "2003    1\n",
       "3214    1\n",
       "Name: doc_id, Length: 262, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[\"doc_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[\"text\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Both agree that the removal of the applicant from Australia may cause his mental condition to deteriorate.                                                                                                                                                                                                                                                                                                                                                                                                                                2\n",
       "If he is to be removed from Australia notwithstanding his mental condition, that removal should take place promptly.                                                                                                                                                                                                                                                                                                                                                                                                                      2\n",
       "Moreover, once removed from Australia, such entitlements as he has, if any, within Australia, would no longer be available to him.                                                                                                                                                                                                                                                                                                                                                                                                        2\n",
       "9 At present, in my view, it is reasonably arguable that it is not reasonably practicable for the applicant to be removed from Australia tomorrow as proposed, because it is likely that his mental condition will very seriously deteriorate by reason of the fact and process of his removal.                                                                                                                                                                                                                                           2\n",
       "That itself introduces a somewhat difficult perspective in the present matter because, unlike other cases in which the health of the person being removed has been considered, the damage to the health of this applicant by the process of removal is related to his fears of what may happen to him if he were to return to Turkey.                                                                                                                                                                                                     2\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ..\n",
       "25 The Music companies had submitted to Moore J that given the evidentiary shortcomings on a subject readily susceptible to documentary demonstration, inclusive of banking records I might add, there was in truth and reality no antecedent loan, that the transfer of those funds by Ms Hemming to TIL in Vanuatu constituted a sham transaction, and consequently that those monies remained her own property beneficially, and should have been identified and disclosed as such in her affidavit provided in the Mareva context.    1\n",
       "Once more, so it was asserted by the Sharman applicants, his Honour declined to make any concluded finding on the subject.                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
       "The point is however that his Honour had been able to infer from the surrounding circumstances I have already outlined that there was some force in the Music companies' submission.                                                                                                                                                                                                                                                                                                                                                      1\n",
       "But in any event his Honour was of the view that he could permit cross-examination of Ms Hemming on and in relation to those matters because at least doubt existed in relation to that area of enquiry. <arguments>                                                                                                                                                                                                                                                                                                                      1\n",
       "I certify that the preceding ten (10) numbered paragraphs are a true copy of the Reasons for Judgment herein of the Honourable Justice Moore.                                                                                                                                                                                                                                                                                                                                                                                             1\n",
       "Name: text, Length: 262, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[\"text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.drop_duplicates(subset=[\"doc_id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1835</td>\n",
       "      <td>1 The applicants Sharman Networks Ltd ('Sharma...</td>\n",
       "      <td>[[17, 37, Organization], [38, 58, Organization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1836</td>\n",
       "      <td>When referring to the applicants generally, I ...</td>\n",
       "      <td>[[60, 84, person]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1837</td>\n",
       "      <td>Each of the Sharman applicants was one of ten ...</td>\n",
       "      <td>[[12, 30, person], [134, 157, Organization], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1838</td>\n",
       "      <td>Wilcox J made orders ancillary to the Mareva o...</td>\n",
       "      <td>[[55, 68, Date], [91, 109, person], [0, 8, Jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1839</td>\n",
       "      <td>2 Wilcox J delivered judgment on the complex i...</td>\n",
       "      <td>[[103, 119, Date], [122, 186, Versus], [2, 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>3208</td>\n",
       "      <td>The short answer to this argument is that it w...</td>\n",
       "      <td>[[120, 126, Location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>3209</td>\n",
       "      <td>To the contrary, he indicated in his applicati...</td>\n",
       "      <td>[[104, 110, Location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>3211</td>\n",
       "      <td>While it appears his position on this issue wa...</td>\n",
       "      <td>[[175, 181, Location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>3212</td>\n",
       "      <td>Even if the Tribunal erred in construing the t...</td>\n",
       "      <td>[[194, 200, Location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>3214</td>\n",
       "      <td>I certify that the preceding ten (10) numbered...</td>\n",
       "      <td>[[127, 141, person]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                                               text  \\\n",
       "0      1835  1 The applicants Sharman Networks Ltd ('Sharma...   \n",
       "1      1836  When referring to the applicants generally, I ...   \n",
       "2      1837  Each of the Sharman applicants was one of ten ...   \n",
       "3      1838  Wilcox J made orders ancillary to the Mareva o...   \n",
       "4      1839  2 Wilcox J delivered judgment on the complex i...   \n",
       "..      ...                                                ...   \n",
       "257    3208  The short answer to this argument is that it w...   \n",
       "258    3209  To the contrary, he indicated in his applicati...   \n",
       "259    3211  While it appears his position on this issue wa...   \n",
       "260    3212  Even if the Tribunal erred in construing the t...   \n",
       "261    3214  I certify that the preceding ten (10) numbered...   \n",
       "\n",
       "                                              entities  \n",
       "0    [[17, 37, Organization], [38, 58, Organization...  \n",
       "1                                   [[60, 84, person]]  \n",
       "2    [[12, 30, person], [134, 157, Organization], [...  \n",
       "3    [[55, 68, Date], [91, 109, person], [0, 8, Jud...  \n",
       "4    [[103, 119, Date], [122, 186, Versus], [2, 10,...  \n",
       "..                                                 ...  \n",
       "257                             [[120, 126, Location]]  \n",
       "258                             [[104, 110, Location]]  \n",
       "259                             [[175, 181, Location]]  \n",
       "260                             [[194, 200, Location]]  \n",
       "261                               [[127, 141, person]]  \n",
       "\n",
       "[262 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[\"text\"]=raw_df[\"text\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 262 entries, 0 to 261\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   doc_id    262 non-null    int64 \n",
      " 1   text      262 non-null    string\n",
      " 2   entities  262 non-null    object\n",
      "dtypes: int64(1), object(1), string(1)\n",
      "memory usage: 8.2+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1835</td>\n",
       "      <td>1 The applicants Sharman Networks Ltd ('Sharma...</td>\n",
       "      <td>[[17, 37, Organization], [38, 58, Organization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1836</td>\n",
       "      <td>When referring to the applicants generally, I ...</td>\n",
       "      <td>[[60, 84, person]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1837</td>\n",
       "      <td>Each of the Sharman applicants was one of ten ...</td>\n",
       "      <td>[[12, 30, person], [134, 157, Organization], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1838</td>\n",
       "      <td>Wilcox J made orders ancillary to the Mareva o...</td>\n",
       "      <td>[[55, 68, Date], [91, 109, person], [0, 8, Jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1839</td>\n",
       "      <td>2 Wilcox J delivered judgment on the complex i...</td>\n",
       "      <td>[[103, 119, Date], [122, 186, Versus], [2, 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>3208</td>\n",
       "      <td>The short answer to this argument is that it w...</td>\n",
       "      <td>[[120, 126, Location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>3209</td>\n",
       "      <td>To the contrary, he indicated in his applicati...</td>\n",
       "      <td>[[104, 110, Location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>3211</td>\n",
       "      <td>While it appears his position on this issue wa...</td>\n",
       "      <td>[[175, 181, Location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>3212</td>\n",
       "      <td>Even if the Tribunal erred in construing the t...</td>\n",
       "      <td>[[194, 200, Location]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>3214</td>\n",
       "      <td>I certify that the preceding ten (10) numbered...</td>\n",
       "      <td>[[127, 141, person]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                                               text  \\\n",
       "0      1835  1 The applicants Sharman Networks Ltd ('Sharma...   \n",
       "1      1836  When referring to the applicants generally, I ...   \n",
       "2      1837  Each of the Sharman applicants was one of ten ...   \n",
       "3      1838  Wilcox J made orders ancillary to the Mareva o...   \n",
       "4      1839  2 Wilcox J delivered judgment on the complex i...   \n",
       "..      ...                                                ...   \n",
       "257    3208  The short answer to this argument is that it w...   \n",
       "258    3209  To the contrary, he indicated in his applicati...   \n",
       "259    3211  While it appears his position on this issue wa...   \n",
       "260    3212  Even if the Tribunal erred in construing the t...   \n",
       "261    3214  I certify that the preceding ten (10) numbered...   \n",
       "\n",
       "                                              entities  \n",
       "0    [[17, 37, Organization], [38, 58, Organization...  \n",
       "1                                   [[60, 84, person]]  \n",
       "2    [[12, 30, person], [134, 157, Organization], [...  \n",
       "3    [[55, 68, Date], [91, 109, person], [0, 8, Jud...  \n",
       "4    [[103, 119, Date], [122, 186, Versus], [2, 10,...  \n",
       "..                                                 ...  \n",
       "257                             [[120, 126, Location]]  \n",
       "258                             [[104, 110, Location]]  \n",
       "259                             [[175, 181, Location]]  \n",
       "260                             [[194, 200, Location]]  \n",
       "261                               [[127, 141, person]]  \n",
       "\n",
       "[262 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into BIO format\n",
    "def convert_to_bio(text, entities):\n",
    "    words = text.split()\n",
    "    bio_tags = [\"O\"] * len(words)\n",
    "    for start, end, entity_type in entities:\n",
    "        s = text[start:end]\n",
    "        for i,word in enumerate(words):\n",
    "            if word is s[0]:\n",
    "                bio_tags[i] = entity_type\n",
    "            elif word in s:\n",
    "                bio_tags[i] = entity_type\n",
    "    return words, bio_tags\n",
    "\n",
    "# Create empty lists for tokens and tags\n",
    "tokens_list = []\n",
    "tags_list = []\n",
    "\n",
    "# Iterate through the rows and convert to BIO format\n",
    "for index, row in raw_df.iterrows():\n",
    "    tokens, tags = convert_to_bio(row[\"text\"], row[\"entities\"])\n",
    "    tokens_list.append(tokens)\n",
    "    tags_list.append(tags)\n",
    "\n",
    "# Add the lists to the DataFrame\n",
    "raw_df[\"tokens\"] = tokens_list\n",
    "raw_df[\"tags\"] = tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_df = raw_df.drop(columns=[\"doc_id\",\"tokens\",\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n",
      "14 14\n",
      "42 42\n",
      "56 56\n",
      "35 35\n",
      "105 105\n",
      "36 36\n",
      "53 53\n",
      "63 63\n",
      "20 20\n",
      "37 37\n",
      "33 33\n",
      "15 15\n",
      "42 42\n",
      "80 80\n",
      "67 67\n",
      "26 26\n",
      "22 22\n",
      "42 42\n",
      "52 52\n",
      "45 45\n",
      "78 78\n",
      "44 44\n",
      "47 47\n",
      "13 13\n",
      "49 49\n",
      "64 64\n",
      "68 68\n",
      "48 48\n",
      "67 67\n",
      "29 29\n",
      "69 69\n",
      "80 80\n",
      "47 47\n",
      "53 53\n",
      "36 36\n",
      "22 22\n",
      "42 42\n",
      "57 57\n",
      "45 45\n",
      "65 65\n",
      "71 71\n",
      "53 53\n",
      "20 20\n",
      "47 47\n",
      "48 48\n",
      "42 42\n",
      "36 36\n",
      "44 44\n",
      "33 33\n",
      "34 34\n",
      "33 33\n",
      "75 75\n",
      "67 67\n",
      "64 64\n",
      "51 51\n",
      "26 26\n",
      "26 26\n",
      "50 50\n",
      "41 41\n",
      "38 38\n",
      "14 14\n",
      "26 26\n",
      "59 59\n",
      "36 36\n",
      "42 42\n",
      "34 34\n",
      "43 43\n",
      "35 35\n",
      "24 24\n",
      "101 101\n",
      "37 37\n",
      "38 38\n",
      "14 14\n",
      "22 22\n",
      "52 52\n",
      "64 64\n",
      "57 57\n",
      "55 55\n",
      "39 39\n",
      "34 34\n",
      "28 28\n",
      "57 57\n",
      "37 37\n",
      "79 79\n",
      "45 45\n",
      "15 15\n",
      "59 59\n",
      "41 41\n",
      "57 57\n",
      "110 110\n",
      "91 91\n",
      "82 82\n",
      "21 21\n",
      "30 30\n",
      "38 38\n",
      "30 30\n",
      "24 24\n",
      "92 92\n",
      "20 20\n",
      "40 40\n",
      "30 30\n",
      "38 38\n",
      "57 57\n",
      "99 99\n",
      "63 63\n",
      "38 38\n",
      "67 67\n",
      "53 53\n",
      "43 43\n",
      "38 38\n",
      "41 41\n",
      "65 65\n",
      "56 56\n",
      "12 12\n",
      "58 58\n",
      "40 40\n",
      "26 26\n",
      "7 7\n",
      "10 10\n",
      "15 15\n",
      "21 21\n",
      "11 11\n",
      "7 7\n",
      "10 10\n",
      "8 8\n",
      "110 110\n",
      "22 22\n",
      "25 25\n",
      "29 29\n",
      "65 65\n",
      "44 44\n",
      "5 5\n",
      "10 10\n",
      "10 10\n",
      "12 12\n",
      "13 13\n",
      "30 30\n",
      "47 47\n",
      "14 14\n",
      "39 39\n",
      "58 58\n",
      "19 19\n",
      "92 92\n",
      "36 36\n",
      "30 30\n",
      "14 14\n",
      "19 19\n",
      "52 52\n",
      "21 21\n",
      "21 21\n",
      "26 26\n",
      "33 33\n",
      "24 24\n",
      "18 18\n",
      "19 19\n",
      "57 57\n",
      "21 21\n",
      "17 17\n",
      "24 24\n",
      "43 43\n",
      "44 44\n",
      "29 29\n",
      "24 24\n",
      "43 43\n",
      "10 10\n",
      "58 58\n",
      "36 36\n",
      "45 45\n",
      "13 13\n",
      "55 55\n",
      "33 33\n",
      "49 49\n",
      "51 51\n",
      "20 20\n",
      "51 51\n",
      "16 16\n",
      "22 22\n",
      "47 47\n",
      "25 25\n",
      "23 23\n",
      "27 27\n",
      "82 82\n",
      "57 57\n",
      "12 12\n",
      "25 25\n",
      "21 21\n",
      "20 20\n",
      "13 13\n",
      "26 26\n",
      "14 14\n",
      "44 44\n",
      "92 92\n",
      "41 41\n",
      "13 13\n",
      "24 24\n",
      "7 7\n",
      "61 61\n",
      "21 21\n",
      "38 38\n",
      "51 51\n",
      "30 30\n",
      "26 26\n",
      "38 38\n",
      "38 38\n",
      "27 27\n",
      "12 12\n",
      "34 34\n",
      "81 81\n",
      "107 107\n",
      "21 21\n",
      "46 46\n",
      "30 30\n",
      "28 28\n",
      "73 73\n",
      "19 19\n",
      "55 55\n",
      "24 24\n",
      "37 37\n",
      "43 43\n",
      "18 18\n",
      "47 47\n",
      "14 14\n",
      "17 17\n",
      "17 17\n",
      "39 39\n",
      "63 63\n",
      "58 58\n",
      "84 84\n",
      "17 17\n",
      "56 56\n",
      "22 22\n",
      "62 62\n",
      "7 7\n",
      "37 37\n",
      "58 58\n",
      "49 49\n",
      "43 43\n",
      "21 21\n",
      "21 21\n",
      "18 18\n",
      "24 24\n",
      "33 33\n",
      "32 32\n",
      "8 8\n",
      "27 27\n",
      "26 26\n",
      "26 26\n",
      "4 4\n",
      "31 31\n",
      "30 30\n",
      "36 36\n",
      "24 24\n",
      "28 28\n",
      "41 41\n",
      "35 35\n",
      "60 60\n",
      "25 25\n",
      "20 20\n",
      "33 33\n",
      "38 38\n",
      "24 24\n"
     ]
    }
   ],
   "source": [
    "for i,row in raw_df.iterrows():\n",
    "    print(len(raw_df[\"tokens\"][i]),len(raw_df[\"tags\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=64):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        labels = self.data.iloc[idx]['tags']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        labels = [labels_map[tag] for tag in labels]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Map labels to numerical IDs\n",
    "labels_map = {'O': 0, 'Organization': 1, 'person': 2, 'Date': 3, 'Judges': 4, 'Versus': 5, 'Location': 6, 'Act&Regulation':7,'Acts&Regulation':8, 'Person':9}\n",
    "\n",
    "# Create the NERDataset\n",
    "dataset = NERDataset(try_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4f44a758484cbfa92d07d1b0445613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 14 at dim 1 (got 63)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Smartsense\\Legal_Entities_From_Legal_Documents\\usage.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Smartsense/Legal_Entities_From_Legal_Documents/usage.ipynb#Y105sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Smartsense/Legal_Entities_From_Legal_Documents/usage.ipynb#Y105sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Smartsense/Legal_Entities_From_Legal_Documents/usage.ipynb#Y105sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Smartsense/Legal_Entities_From_Legal_Documents/usage.ipynb#Y105sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     data_collator\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# You can specify a data collator if needed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Smartsense/Legal_Entities_From_Legal_Documents/usage.ipynb#Y105sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mdataset\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Smartsense/Legal_Entities_From_Legal_Documents/usage.ipynb#Y105sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Smartsense/Legal_Entities_From_Legal_Documents/usage.ipynb#Y105sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Fine-tune the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Smartsense/Legal_Entities_From_Legal_Documents/usage.ipynb#Y105sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Smartsense/Legal_Entities_From_Legal_Documents/usage.ipynb#Y105sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n",
      "File \u001b[1;32mc:\\Users\\chava\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\chava\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:1815\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1812\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1815\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1816\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1817\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mc:\\Users\\chava\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[0;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chava\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\chava\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\chava\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\chava\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer_utils.py:707\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[\u001b[39mdict\u001b[39m]):\n\u001b[0;32m    706\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remove_columns(feature) \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features]\n\u001b[1;32m--> 707\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_collator(features)\n",
      "File \u001b[1;32mc:\\Users\\chava\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\data_collator.py:70\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[0;32m     71\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32mc:\\Users\\chava\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\data_collator.py:136\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    134\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mstack([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features]))\n\u001b[0;32m    135\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([f[k] \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m features])\n\u001b[0;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 14 at dim 1 (got 63)"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_map))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./ner_model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Create a Trainer instance for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=None,  # You can specify a data collator if needed\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    f = open(filename)\n",
    "    data = []\n",
    "    sentence = []\n",
    "    label= []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence,label))\n",
    "                sentence = []\n",
    "                label = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append(splits[0])\n",
    "        label.append(splits[-1][:-1])\n",
    "\n",
    "    if len(sentence) >0:\n",
    "        data.append((sentence,label))\n",
    "        sentence = []\n",
    "        label = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = readfile(\"datasets/legal_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = readfile(\"datasets/legal_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = readfile(\"datasets/legal_valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
